{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "from deep_translator import GoogleTranslator\n",
    "from nanoid import generate\n",
    "from slugify import slugify\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL: https://www.epam.com/careers/job-listings?country=Mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.epam.com/services/vacancy/search?locale=en&limit=200&recruitingUrl=%2Fcontent%2Fepam%2Fen%2Fcareers%2Fjob-listings%2Fjob&query=&country=Mexico&sort=relevance&offset=0&searchType=placeOfWorkFilter&_=1709566523032')\n",
    "\n",
    "dictionario = response.json()\n",
    "\n",
    "print(dictionario)\n",
    "\n",
    "print(len(dictionario['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_contenido_div(url):\n",
    "    # Realizamos la solicitud GET a la URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Verificamos si la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Parseamos el contenido HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Encontramos el div con la clase deseada\n",
    "        div_contenido = soup.find('div', class_='vacancy-details-23__content-holder')  \n",
    "        \n",
    "        # Verificamos si se encontró el div\n",
    "        if div_contenido:\n",
    "            # Inicializamos una lista para almacenar el contenido\n",
    "            contenido = []\n",
    "            \n",
    "            # Recorremos todos los elementos <h3> dentro del div\n",
    "            for h3_tag in div_contenido.find_all('h3'):\n",
    "                # Agregamos el texto de cada título a la lista de contenido\n",
    "                contenido.append(h3_tag.text.strip())\n",
    "            \n",
    "            return contenido\n",
    "        else:\n",
    "            print(\"No se encontró el div con la clase especificada.\")\n",
    "    else:\n",
    "        print(\"Error al obtener la página:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página a scrapear\n",
    "url = 'https://www.epam.com/careers/job-listings/job.95046.senior-support-project-administrator_guadalajara_mexico'\n",
    "\n",
    "# Llamamos a la función y mostramos el contenido extraído\n",
    "contenido_div = extraer_contenido_div(url)\n",
    "if contenido_div:\n",
    "    print(\"Contenido del div:\")\n",
    "    for item in contenido_div:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.epam.com/careers/job-listings/job.95046.senior-support-project-administrator_guadalajara_mexico'\n",
    "# Parseamos el contenido HTML\n",
    "soup = BeautifulSoup(url, 'html.parser')\n",
    "\n",
    "# Encontramos el div con la clase \"vacancy-details-23__content-holder\"\n",
    "div_contenido = soup.find('div', class_='vacancy-details-23__content-holder')\n",
    "\n",
    "# Verificamos si se encontró el div\n",
    "if div_contenido:\n",
    "    # Inicializamos un diccionario para almacenar el contenido\n",
    "    contenido_por_titulo = {}\n",
    "    \n",
    "    # Recorremos todos los elementos <h3> dentro del div\n",
    "    for h3_tag in div_contenido.find_all('h3'):\n",
    "        # Inicializamos una lista para almacenar los elementos de lista correspondientes a este título\n",
    "        elementos_li = []\n",
    "        \n",
    "        # Buscamos el siguiente elemento <ul> hermano del título <h3>\n",
    "        ul_tag = h3_tag.find_next_sibling('ul')\n",
    "        \n",
    "        # Recorremos todos los elementos <li> dentro del <ul>\n",
    "        for li_tag in ul_tag.find_all('li'):\n",
    "            # Agregamos el texto del elemento <li> a la lista de elementos_li\n",
    "            elementos_li.append(li_tag.text.strip())\n",
    "        \n",
    "        # Almacenamos los elementos de lista en el diccionario bajo el título correspondiente\n",
    "        contenido_por_titulo[h3_tag.text.strip()] = elementos_li\n",
    "\n",
    "# Mostramos el contenido almacenado en variables con los nombres de los títulos\n",
    "for titulo, elementos in contenido_por_titulo.items():\n",
    "    # Creamos una variable con el nombre del título y le asignamos la lista de elementos\n",
    "    globals()[titulo.replace(\" \", \"_\").lower()] = elementos\n",
    "\n",
    "# Ahora puedes acceder a cada lista de elementos utilizando el nombre de la variable correspondiente al título\n",
    "print(\"Contenido almacenado en variables:\")\n",
    "print(\"Responsibilities:\", responsibilities)\n",
    "print(\"Requirements:\", requirements)\n",
    "print(\"Technologies:\", technologies)\n",
    "print(\"We_Offer:\", we_offer)\n",
    "print(\"Conditions:\", conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_contenido_h3_ul_desde_url(url):\n",
    "    try:\n",
    "        # Realizamos la solicitud GET a la URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Verificamos si la solicitud fue exitosa\n",
    "        if response.status_code == 200:\n",
    "            # Parseamos el contenido HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Encontramos el div con la clase \"vacancy-details-23__content-holder\"\n",
    "            div_contenido = soup.find('div', class_='vacancy-details-23__content-holder')\n",
    "            \n",
    "            # Verificamos si se encontró el div\n",
    "            if div_contenido:\n",
    "                # Inicializamos un diccionario para almacenar el contenido\n",
    "                contenido_por_titulo = {}\n",
    "                \n",
    "                # Recorremos todos los elementos <h3> dentro del div\n",
    "                for h3_tag in div_contenido.find_all('h3'):\n",
    "                    # Inicializamos una lista para almacenar los elementos de lista correspondientes a este título\n",
    "                    elementos_li = []\n",
    "                    \n",
    "                    # Buscamos el siguiente elemento <ul> hermano del título <h3>\n",
    "                    ul_tag = h3_tag.find_next_sibling('ul')\n",
    "                    \n",
    "                    # Recorremos todos los elementos <li> dentro del <ul>\n",
    "                    for li_tag in ul_tag.find_all('li'):\n",
    "                        # Agregamos el texto del elemento <li> a la lista de elementos_li\n",
    "                        elementos_li.append(li_tag.text.strip())\n",
    "                    \n",
    "                    # Almacenamos los elementos de lista en el diccionario bajo el título correspondiente\n",
    "                    contenido_por_titulo[h3_tag.text.strip()] = elementos_li\n",
    "                \n",
    "                return contenido_por_titulo\n",
    "            else:\n",
    "                print(\"No se encontró el div con la clase especificada.\")\n",
    "        else:\n",
    "            print(\"Error al obtener la página:\", response.status_code)\n",
    "    except Exception as e:\n",
    "        print(\"Ocurrió un error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "url = 'https://www.epam.com/careers/job-listings/job.95046.senior-support-project-administrator_guadalajara_mexico'\n",
    "contenido_por_titulo = extraer_contenido_h3_ul_desde_url(url)\n",
    "\n",
    "if contenido_por_titulo:\n",
    "    # Mostramos el contenido almacenado en el diccionario\n",
    "    for titulo, elementos in contenido_por_titulo.items():\n",
    "        print(f\"{titulo}:\")\n",
    "        for elemento in elementos:\n",
    "            print(f\"- {elemento}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epam(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"name\"] = clean_title(df[\"name\"])\n",
    "    df[\"companyName\"] = df[\"companyName\"].replace(df[\"companyName\"].values, \"Apple\")\n",
    "    df[\"Otros_requisitos\"] = df[\"Otros_requisitos\"].fillna(\"\")\n",
    "    df[\"Educación_Experiencia\"] = df[\"Educación_Experiencia\"].fillna(\"\")\n",
    "    columns_to_merge = [\n",
    "        \"Responsabilities\",\n",
    "        \"Requirements\",\n",
    "        \"We Offer\",\n",
    "        \"conditions\"\n",
    "    ]\n",
    "    column_merge_new = \"Req_txt\"\n",
    "    columns_to_delete = [\n",
    "        \"Responsabilities\",\n",
    "        \"Requirements\",\n",
    "        \"conditions\"\n",
    "    ]\n",
    "    df = merge_clean_delete(df, column_merge_new, columns_to_merge, columns_to_delete)\n",
    "    df[\"skills\"] = df[\"Req_txt\"].apply(find_skills)\n",
    "    df[\"skills\"] = df[\"skills\"].drop_duplicates(keep=\"last\")\n",
    "    df[\"skills\"] = df[\"skills\"].apply(dict_skills)\n",
    "    df[\"tools\"] = df[\"Req_txt\"].apply(find_tools)\n",
    "    df[\"tools\"] = df[\"tools\"].apply(dict_tools)\n",
    "    df[\"tools\"] = df[\"tools\"].drop_duplicates(keep=\"last\")\n",
    "    df[\"aptitudes\"] = df[\"Req_txt\"].apply(find_aptitudes)\n",
    "    df[\"aptitudes\"] = df[\"aptitudes\"].drop_duplicates(keep=\"last\")\n",
    "    df[\"aptitudes\"] = df[\"aptitudes\"].apply(dict_aptitudes)\n",
    "    df[\"languages\"] = df[\"Req_txt\"].apply(find_languages)\n",
    "    df[\"languages\"] = df[\"languages\"].drop_duplicates(keep=\"last\")\n",
    "    df[\"languages\"] = df[\"languages\"].apply(dict_languages)\n",
    "    df = df.drop(\"Req_txt\", axis=1)\n",
    "    df = translate_columns_deep_translator(\n",
    "        df, column_names=[\"aptitudes\", \"languages\", \"skills\"]\n",
    "    )\n",
    "    df = complete_dataframe(df)\n",
    "    df = fill_data(df, companyId=\"epam\")\n",
    "    df[\"skills\"] = df[\"skills\"].apply(json_nan)\n",
    "    df[\"tools\"] = df[\"tools\"].apply(json_nan)\n",
    "    df[\"aptitudes\"] = df[\"aptitudes\"].apply(json_nan)\n",
    "    df[\"languages\"] = df[\"languages\"].apply(json_nan)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_epam = {\n",
    "    \"userId\": \"epam\",\n",
    "    \"name\": \"epam\",\n",
    "    \"description\": \"\"\"\n",
    "                    EPAM Systems, Inc. es una empresa estadounidense que se especializa en servicios de ingeniería de software, ingeniería de plataformas digitales y diseño de productos digitales, que opera en Newtown, Pensilvania. EPAM es miembro fundador de la Alianza MACH.\n",
    "                    \"\"\",\n",
    "    \"website\": \"www.epam.com//\",\n",
    "    \"sector\": \"Tecnologías de la información y la comunicación\",\n",
    "    \"isExternal\": True,\n",
    "}\n",
    "\n",
    "df_epam = pd.DataFrame(diccionario_epam, index=[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
